{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification using convolutional neural networks\n",
    "\n",
    "Audio classification can be performed by converting audio streams into [spectrograms](https://en.wikipedia.org/wiki/Spectrogram), which provide visual representations of spectrums of frequencies as they vary over time, and classifying the spectrograms using [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs). \n",
    "\n",
    "This notebook is an adaptation of https://github.com/jeffprosise/Deep-Learning/blob/master/Audio%20Classification%20(CNN).ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate spectrograms\n",
    "\n",
    "\n",
    "The first step is to load the WAV files, use a Python package named [Librosa](https://librosa.org/) to generate spectrogram images from them, load the spectrograms into memory, and prepare them for use in training a CNN. To aid in this process, we'll define a pair of helper functions for creating spectrograms from WAV files and converting all the WAV files in a specified directory into spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa.display, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def create_spectrogram(audio_file, image_file):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    ms = librosa.feature.melspectrogram(y, sr=sr)\n",
    "    log_ms = librosa.power_to_db(ms, ref=np.max)\n",
    "    librosa.display.specshow(log_ms, sr=sr)\n",
    "\n",
    "    fig.savefig(image_file)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def create_pngs_from_wavs(input_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    dir = os.listdir(input_path)\n",
    "\n",
    "    for i, file in enumerate(dir):\n",
    "        input_file = os.path.join(input_path, file)\n",
    "        output_file = os.path.join(output_path, file.replace('.wav', '.png'))\n",
    "        create_spectrogram(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate spectrograms with labels\n",
    "\n",
    "The next cell contains a variation from create_spectrogram and create_pngs_from_wavs functions, these variations add frecuency and time labels  for the spectrograms, though the training is done without them, labeled images can be used for better visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_spectrogram(audio_file, image_file):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    ms = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    log_ms = librosa.power_to_db(ms, ref=np.max)\n",
    "    librosa.display.specshow(log_ms, sr=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    img = librosa.display.specshow(log_ms, sr=sr, x_axis=\"time\", y_axis=\"mel\", ax=ax)\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xlabel(\"Time (s)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Frequency (Hz)\", fontsize=12)\n",
    "    fig.savefig(image_file)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def create_labeled_pngs_from_wavs(input_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    dir = os.listdir(input_path)\n",
    "\n",
    "    for i, file in enumerate(dir):\n",
    "        input_file = os.path.join(input_path, file)\n",
    "        output_file = os.path.join(output_path, file.replace('.wav', '.png'))\n",
    "        create_labeled_spectrogram(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PNG files containing spectrograms from all the WAV files in the \"spanish\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pngs_from_wavs('spanish', 'spanish_spectrograms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PNG files containing spectrograms from all the WAV files in the \"mayan\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pngs_from_wavs('mayan', 'mayan_spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two new helper functions for loading and displaying spectrograms and declare two Python lists — one to store spectrogram images, and another to store class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        images.append(image.img_to_array(image.load_img(os.path.join(path, file), target_size=(224, 224, 3))))\n",
    "        labels.append((label))\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "def show_images(images):\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i] / 255)\n",
    "        \n",
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the spanish spectrogram images, add them to the list named `x`, and label them with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('spanish_spectrograms', 0)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mayan spectrogram images, add them to the list named `x`, and label them with 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images_from_path('mayan_spectrograms', 1)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the images and labels into three datasets — one for training, one for testing, and one for validation. Then divide the pixel values by 255 and one-hot-encode the labels using Keras's [to_categorical](https://keras.io/api/utils/python_utils/#to_categorical-function) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split data into training (80%) and a combined group for training and validation (20%)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, stratify=y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Split the combined group (x_temp) into validation and trainig\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "x_train_norm = np.array(x_train) / 255\n",
    "x_val_norm = np.array(x_val) / 255\n",
    "x_test_norm = np.array(x_test) / 255\n",
    "\n",
    "# Codificar las etiquetas\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_val_encoded = to_categorical(y_val)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a CNN with transfer learning\n",
    "\n",
    "State-of-the-art image classification typically isn't done with traditional neural networks. Rather, it is performed with convolutional neural networks that use [convolution layers](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) to extract features from images and [pooling layers](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) to downsize images so features can be detected at various resolutions. The next task is to build a CNN containing a series of convolution and pooling layers for feature extraction, a pair of fully connected layers for classification, and a `softmax` layer that outputs probabilities for each class, and to train it with spectrogram images and labels. Start by defining the CNN.\n",
    "\n",
    "[Transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is a powerful technique that allows sophisticated CNNs trained by Google, Microsoft, and others on GPUs to be repurposed and used to solve domain-specific problems. Many pretrained CNNs are available in the public domain, and several are included with Keras. Let's use [`MobileNetV2`](https://keras.io/api/applications/mobilenet/), a pretrained CNN from Google that is optimized for mobile devices, to extract features from spectrogram images.\n",
    "\n",
    "> `MobileNetV2` requires less processing power and has a smaller memory footprint than CNNs such as `ResNet50V2`. That's why it is ideal for mobile devices. You can learn more about it in the [Google AI blog](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html).\n",
    "\n",
    "Start by calling Keras's [MobileNetV2](https://keras.io/api/applications/mobilenet/) function to instantiate `MobileNetV2` without the classification layers. Use the [preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/preprocess_input) function for `MobileNet` networks to preprocess the training and testing images. Then run both datasets through `MobileNetV2` to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x_train_norm = preprocess_input(np.array(x_train))\n",
    "x_test_norm = preprocess_input(np.array(x_test))\n",
    "x_val_norm = preprocess_input(np.array(x_val))\n",
    "\n",
    "train_features = base_model.predict(x_train_norm)\n",
    "test_features = base_model.predict(x_test_norm)\n",
    "val_features = base_model.predict(x_val_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network to classify features extracted by `MobileNetV2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with features extracted by `MobileNetV2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_features, y_train_encoded, validation_data=(test_features, y_test_encoded), batch_size=10, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the validation images through the network and use a confusion matrix to assess the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "y_predicted = model.predict(val_features)\n",
    "mat = confusion_matrix(y_test_encoded.argmax(axis=1), y_predicted.argmax(axis=1))\n",
    "class_labels = ['español', 'maya']\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('Actual label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy, recall, precision and f1 score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "recall = recall_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving additional info\n",
    "Training is done and we have the metrics, so next we can save the model and the split in case we need that information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "Saving it as a .keras file, allows you use the trained model in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the split\n",
    "Save the split by first creating the subfolders where the splitted images (train, test, validation) will be store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "base_dir = 'split'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Make subfolders\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use save_images function to save the splitted images to their respective subfolder, creating a subfolder for each class in the process\n",
    "\n",
    "NOTE: The images are saved from the array used in the training so original picture names are lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_images(image_list, labels, target_dir):\n",
    "    for idx, (image_array, label) in enumerate(zip(image_list, labels)):\n",
    "        label = str(label)\n",
    "\n",
    "        # Create subfolder for class\n",
    "        class_dir = os.path.join(target_dir, label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        image = Image.fromarray(image_array.astype(np.uint8)) \n",
    "\n",
    "        image_path = os.path.join(class_dir, f\"image_{idx}.png\")\n",
    "        image.save(image_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_images(x_train, y_train, train_dir)\n",
    "save_images(x_test, y_test, test_dir)\n",
    "save_images(x_val, y_val, val_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
